{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('/apps')\n",
    "import django\n",
    "django.setup()\n",
    "from drivers.tree_builder import TreeNeo\n",
    "from drivers.graph_models import TreeNode, Order, Family, graph,Kingdom,Occurrence\n",
    "from drivers.graph_models import Cell,Mex4km, countObjectsOf\n",
    "from drivers.graph_models import pickNode\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "\n",
    "## Use the ggplot style\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Regression for richness of certain taxon\n",
    "---\n",
    "Abstraction dashboard for the joininig the data with the model\n",
    "\n",
    "***\n",
    "Here I show how to extract different taxonomic information at cell level.\n",
    "Although there exists a method for building the taxonomic tree within a single cell, the process can be computationally intensive because it depends on extracting the total amount of occurrences in each cell. From there, it traverses fromtop to bottom the tree looking for the corresponding nodes.\n",
    "\n",
    "The approach is usefull when one needs a small number of trees but it'll become increasingly slow if the amount of cells or occurrences increases. \n",
    "\n",
    "---\n",
    "\n",
    "## Extracting specific taxonomic levels en each cells\n",
    "\n",
    "The method studied here makes use of the relationship type `IS_IN` stored in the knowledge graph.\n",
    "\n",
    "> Developer's note: *There was a problem with the design of the OGM implementation (py2neo.ogm). The retrieval of linked nodes based on a specific relation does not distinguish different labels. In other words it returns the totality of the data that has the  specific relationship given a node.*\n",
    "\n",
    "> Patchy solution: \n",
    "The solution was to include extra methods for the class Cell `has_[taxas]`. This method/attribute returns a graph selector that points to the corresponding nodes.\n",
    "\n",
    "> Stable Fix: \n",
    "Make relationships as specific as possible (given the data). For example, if instead of using the relation type \n",
    "* *IS_IN* for (Bursera:Family) -[IS_IN]-> (Grid:Cell) \n",
    "change it to:\n",
    "* *Family_IS_IN* for (Bursera:Family) -[IS_IN]-> (Grid:Cell)\n",
    "Let's get started.\n",
    "As usual we need to load the necessary modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Let's pick the bats node\n",
    "bats = pickNode(Order,name='Chiroptera')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids4bats = bats.getCellsById()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time cells_w_bats = list(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd = pd.DataFrame(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[o['c.id'] for o in cc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time list(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ccs = list(bats.giveNCells(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = bats.is_in.related_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C.select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Explore why is taking so much time\n",
    "cells = bats.is_in._related_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cells_w_bats,b = zip(*cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random selection of cells.\n",
    "\n",
    "> Note: Data Arquitecture. For storage reasons I couldn't load the complete world bioclimatic layers. Therefore I needed to put a regional subset that comprises only the Mexican Territory. \n",
    "For this reason, it is necessary that any approach for selecting subsamples needs to be constrained (filter) by this geometry. \n",
    "We can do that with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sketches.models import Country\n",
    "from mesh.models import MexMesh\n",
    "\n",
    "Mexico = Country.objects.filter(name__contains=\"exico\").get()\n",
    "mexican_cells = MexMesh.objects.filter(cell__intersects=Mexico.geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain list of cells within the Mexican Territory.\n",
    "> The attribute: `mexican_cells.values` is a generator of the Type: QuerySet. We need to cast it to list for loading all the data in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all cell ids\n",
    "selected_cells = mexican_cells\n",
    "#selected_cells = cells_w_bats\n",
    "#ids = list(selected_cells.values('pk'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UniformRandomCellSample is a method for sampling cells in the example below we give as arguments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract richness and Environmental covariates from cells at a given taxonomic level\n",
    "Options are: Family, Order, Spicies, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from traversals import strategies as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from traversals import sampling as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trees = sm.UniformRandomSampleForest(selected_cells,size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time ts = list(trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time data = st.getEnvironmentalCovariatesFromListOfTrees(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time x = arbol1.associatedData.getEnvironmentalVariablesCells()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arbol1 = ts[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time st.getEnvironmentalCovariatesFromListOfTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time data = st.getEnvironmentAndRichnessFromListOfCells(list_of_cells=selected_cells,taxonomic_level_name='Family')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.loc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.n_Family.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes time because it need to calculate on the fly the summary statistic of each cell. It is using the postgis backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.plot(column='n_Family')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the predictors\n",
    "In this case we will bring all the variables to start working with everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from raster_api.tools import RasterData\n",
    "from raster_api.models import raster_models_dic as models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datadict = { key : RasterData(models[key],border=Mexico.geom) for key,value in models.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Without resamling (whole data)\n",
    "pixel_size = 0.25\n",
    "%time datacube_field = map(lambda raster : raster.rescale(pixel_size),datadict.itervalues())\n",
    "datacube = datacube_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map(lambda (k,d) : d.display_field(title=k,origin='Lower'),datadict.iteritems())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the moment we don't want to do temporal analysis so we need to aggregate the array by the mean.\n",
    "\n",
    "Using the new 'resample' method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take mean of evetything\n",
    "cubes = map(lambda (k,v): np.mean(v.toNumpyArray(),axis=0), datadict.iteritems())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinates are the same so, we can extract the them with getCoordinates and then append everything as a flat array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coords = map(lambda (k,v) : v.getCoordinates(),datadict.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coords = pd.concat(coords,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coords1 = coords[[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Oke I need a way to extract the dataframe, maybe aggregate it by mean \n",
    "dataframe_cube = map(lambda cube: pd.DataFrame(cube.flatten()),cubes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datacube = pd.concat(dataframe_cube,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datacube = pd.concat([datacube,coords1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datacube.columns = datadict.keys() + list(coords1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(datacube.Longitude,datacube.Latitude,c=datacube.SolarRadiation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datacube_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datacube.loc[:10]\n",
    "datacube_clean = datacube.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Convert to geopandas\n",
    "from external_plugins.spystats.spystats import tools as tl\n",
    "datacube_clean = tl.toGeoDataFrame(datacube_clean,xcoord_name='Longitude',ycoord_name='Latitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Modelling\n",
    "\n",
    "The Model is an inhomogeneous Poisson process.\n",
    "\n",
    "$$y(x) \\sim Poisson(\\lambda(x)) $$\n",
    "$$ \\lambda(x) = exp\\{\\alpha + S(x)\\}$$\n",
    "\n",
    "Where $S(x)$ is a Gaussian Process such that:\n",
    "\n",
    "$$S(x) \\sim MVN(0,\\sigma^2 \\rho(||x - x'||) $$\n",
    "\n",
    "For this particular case:\n",
    "$$\\rho = Matern(\\phi,\\kappa = \\frac{3}{4}) + \\tau^2$$\n",
    "\n",
    "I'm using a Bayesian approach with parameters $\\phi$ and $\\tau$ as random variables with it's corresponding priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "data.columns = [u'n_Family', u'Longitude', u'Latitude', u'Elevation_mean',\n",
    "       u'MaxTemperature_mean', u'MeanTemperature_mean', u'MinTemperature_mean',\n",
    "       u'Precipitation_mean', u'SolarRadiation_mean', u'Vapor_mean',\n",
    "       u'WindSpeed_mean', u'geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "glmodel = GLM.from_formula('n_Family ~ Elevation_mean + MaxTemperature_mean',data=data)\n",
    "res = glmodel.fit()\n",
    "print(res.summary())\n",
    "\n",
    "\n",
    "z = np.array([0.0,0.0])\n",
    "coefs = np.append(z,res.params.values[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## cero coef for long lat\n",
    "z = np.array([0.0,0.0])\n",
    "coefs = np.append(z,res.params.values[1:])\n",
    "print(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Analysis, GP only one parameter to fit\n",
    "# The variational method is much beter.\n",
    "from pymc3.variational.callbacks import CheckParametersConvergence\n",
    "\n",
    "with pm.Model() as model:\n",
    "    sigma = 1.0\n",
    "    #range_a=10.13\n",
    "    \n",
    "    \n",
    "    tau = pm.Uniform('tau',0,5.0)\n",
    "    #sigma = pm.Flat('sigma')\n",
    "    #phi = pm.HalfNormal('phi',mu=8,sd=3)\n",
    "    #phi = pm.Uniform('phi',6,12)\n",
    "    phi = pm.Uniform('phi',0,15)\n",
    "    \n",
    "    Tau = pm.gp.cov.Constant(tau)\n",
    "    \n",
    "    cov = sigma * pm.gp.cov.Matern32(2,phi,active_dims=[0,1]) + Tau\n",
    "    #K = cov(grid[['Lon','Lat']].values)\n",
    "    #phiprint = tt.printing.Print('phi')(phi)\n",
    "    \n",
    "    \n",
    "    mf = pm.gp.mean.Linear(coeffs=coefs,intercept=res.params.values[0])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## The latent function\n",
    "    gp = pm.gp.Latent(cov_func=cov)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## I don't know why this\n",
    "    #f = gp.prior(\"latent_field\", X=data[['Longitude','Latitude']].values,reparameterize=False)\n",
    "    \n",
    "    f = gp.prior(\"latent_field\", X=data[['Longitude','Latitude','Elevation_mean','MaxTemperature_mean']].values,reparameterize=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #f_print = tt.printing.Print('latent_field')(f)\n",
    "    \n",
    "    y_obs = pm.Poisson('y_obs',mu=np.exp(f),observed=data[['n_Family']].values)\n",
    "    \n",
    "    #y_obs = pm.MvNormal('y_obs',mu=np.zeros(n*n),cov=K,observed=grid.Z)\n",
    "\n",
    "    #gp = pm.gp.Latent(cov_func=cov,observed=sample)\n",
    "    # Use elliptical slice sampling\n",
    "    #ess_step = pm.EllipticalSlice(vars=[f_sample], prior_cov=K)\n",
    "    #step = pm.HamiltonianMC()\n",
    "    #step = pm.Metropolis()\n",
    "    #%time trace = pm.sample(5000,step)#,tune=0,chains=1)\n",
    "    ## Variational\n",
    "    \n",
    "    %time mean_field = pm.fit(method='advi', callbacks=[CheckParametersConvergence()],n=15000)    \n",
    "    %time trace = mean_field.sample(draws=5000)\n",
    "\n",
    "#with model:    \n",
    "    \n",
    "    ## For predicting\n",
    "    #%time f_star = gp.conditional(\"f_star\", data_star.iloc[:,1:3].values)\n",
    "    #%time f_star = gp.conditional(\"f_star\", small_sample.iloc[:,1:3].values)\n",
    "    %time f_star = gp.conditional(\"f_star\", datacube_clean[['Longitude','Latitude','Elevation','MeanTemperature']].values)\n",
    "\n",
    "    ## Full data\n",
    "    ##%time f_star = gp.conditional(\"f_star\",elev_data.iloc[:,1:3].values)\n",
    "\n",
    "\n",
    "    \n",
    "#with model:\n",
    "    ## sampling predictions posterior predictive checks\n",
    "    pred_samples = pm.sample_ppc(trace, vars=[f_star], samples=10)\n",
    "\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(pred_samples['f_star']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds['mean_sample'] = preds.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preds['idx'] = data_star.index.values\n",
    "preds['idx'] = datacube_clean.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#test1 = data_s.merge(preds,how='left',left_index=True,right_on='idx',suffixes=('_obs','_pred'))\n",
    "test1 = datacube.merge(preds,how='left',left_index=True,right_on='idx',suffixes=('_obs','_pred'))\n",
    "## Only the values of small_sample\n",
    "#test2 = elev_data.merge(preds,how='inner',left_index=True,right_on='idx',suffixes=('_obs','_pred'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 9));\n",
    "plt.scatter(test1.Longitude,test1.Latitude,c=test1.mean_sample)\n",
    "plt.scatter(data.Longitude,data.Latitude,c=data.n_Family,cmap=plt.cm.Greys,s=90)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to a raster format.\n",
    " ## Motivation\n",
    " It's important for visualization and compatibility with GIS software to generate the results in a standard raster format.\n",
    "*Biospytial* Has incorporated tools for reading and converting to the standard raster formats. In this case Geotif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import raster container\n",
    "from raster_api.tools import RasterContainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The easiest way is to take the metadata (geospatial parameters) from one of the RasterData we used as covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elv_rast = datadict['Elevation'].rasterdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_data = test1.mean_sample.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ncounts_families = RasterContainer(predicted_data,use_metadata_from=elv_rast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ncounts_families.display_field(band=1,origin='Lower',title='log(family richness)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ncounts_families.display_field(band=2,origin='Lower',title='family richness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to Geotif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ncounts_families.exportToGeoTiff('ncount_families_elev_meantemps_2.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several parameters hidden here:\n",
    "    1. The size of the predictors grid\n",
    "    2. The sample size of the training data\n",
    "    3. The bayesian hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## For implementing today\n",
    "* The modeller function (structure):\n",
    "    \n",
    "   * Inputs: \n",
    "    \n",
    "    ** Dataframe for training data\n",
    "    \n",
    "    ** numpy n-array or dataframe for predictors\n",
    "    \n",
    "    ** Model (specified as Pymc3 model)\n",
    "    \n",
    "    * Outputs:\n",
    "        \n",
    "        *** The new RasterContainer with rthe bands as: log, exp and expit (in case binomial)\n",
    "        \n",
    "* Seems like needs to be a premodelr to select the cells, the geometry, etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ncounts_families.rasterdata.bands[0].data().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stages\n",
    "1. Build a function for extracting the predictors given a scale parameter\n",
    "2. Build a function for the \"premodeling\"\n",
    "3. Build a function for bundling everything and return the prediction (needs to accept a model)\n",
    "\n",
    "> Models to run\n",
    "Show two maps of taxa. e.g. Agave and Bats for instance\n",
    "\n",
    "Then also show the matrix distance for trees \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
